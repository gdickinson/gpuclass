\documentclass[twocolumn]{article}
\usepackage{fullpage}
\usepackage{mathpazo}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{inconsolata}

\begin{document}
  
  \title{GPU Project Final Report: Accelerated RC4 Stream Cipher}
  \author{Guy Dickinson (guy.dickinson@nyu.edu) \& William Ward, (wwward@nyu.edu) \\ New York University, Department of Computer Science}
  \maketitle
  
  \section{Introduction/Abstract}
  The RC4\footnote{RC4 remains a trademark of RSA; RC4 is alternately known as ARCFOUR but referred to here as RC4 for clarity.} cipher is a widely used stream cipher found in many common applications, from WEP/TKIP \cite[p. 171]{cisco-netsec} to OpenSSL \cite{openssl}. It is a stream cipher with a particularly simple encipher/decipher operation; the actual mutation of plaintext to ciphertext is a simple bitwise XOR operation against a keystream generated from a relatively straightforward state machine initialized with a secret key of arbitrary length.
  
  The specification for RC4 has never officially been released by its corporate owner, RSA, although its creator, Ron Rivest, has tacitly acknowledged veracity of the public implementations of RSA by linking to the Wikipedia page for RC4 in his course notes at MIT.\cite{rivest-notes}
  
  Previous work in this area \cite{5276924} was largely been limited to a proof-of-concept implementation which encrypted and decrypted static files on-disk. This gave the authors the luxury of knowing exactly how much data is to be encrypted before the program is run. Such is not the life a stream cipher in the field; a stream cipher must be able to take arbitrary amounts of data at any given moment and efficiently process it. Our implementation efficiently streams data to and from the GPU making use of CUDA's multi-stream instruction architecture. 
  
  This paper describes a simple but effective strategy for a stream-based, highly parallelized implementation of RC4 using CUDA, which leads to an up to 40\% increase in speed over an equivalent serial version under optimal conditions.
  
  \section{Design}
  
  Our RC4 implementation has two major operations and three major components. The components are:
  
  \begin{itemize}
    \item An array, $A_t$ of arbitrary length $l$  bytes containing the target data to be encrypted or decrypted.
    \item A second array $A_k$, also of length $l$ bytes, containing the keystream with which the data is to be encrypted.
    \item A data structure $M$ which represents the current state of the keystream generator. That data structure contains:
    \begin{itemize}
      \item A an array of all 256 possible bytes (that is to say, all possible 8-bit binary combinations, $2^8=256$ in total).
      \item Two pointers, $i$ and $j$ which are used to exchange bytes in the permutation and return a byte for encryption/decryption purposes.
    \end{itemize}
  \end{itemize}
  
  The operations are:
  
  \begin{itemize}
    \item Initialization of the RC4 state.
    \item Encryption/decryption of data.
  \end{itemize}
  
  Initialization is performed only once per run of the RC4 algorithm and takes negligible time. Encryption/decryption is performed with the simple operation $A_k \oplus A_t$
  
  RC4 presents an interesting challenge from a parallelization perspective: although the actual bit-flipping operation can be parallelized very easily, generating the keystream is an inherently parallel operation. That is to say, the operation of calculating byte $n$ of the keystream explicitly requires byte $n-1$ to perform.
  
  Because generating the keystream is so inherently parallel, trying to contort the GPU into performing this task made no sense; the key-scheduler and its operation remains on the CPU. All encryption and decryption is done on the GPU.
  
  Instead of reading from a file using C's \texttt{fread()} functions, we take input from the standard input stream, to simulate an environment where data arrives one byte at a time and the total length of the data is not known; EOF may be reached at any time.
    
  \subsection{Partitioning}
  Partitioning of data is somewhat trivial for RC4 since the structures are simply one-dimensional arrays of a known length:
  
  Data is captured into a buffer of fixed length $l$, specified at run-time. During testing, we used a buffer size of 512, but this can be increased arbitrarily. When the buffer is full, it is flushed and encrypted against a chunk of keystream also of length $l$.
  
  
  \subsection{GPU Grid Geometry}
  The GPU grid geometry is decided using a trivial algorithm. We can ``get away'' with this because the kernel used in this application is so small; there is no possibility for thread divergence since, in essence, each thread performs a single read, \texttt{XOR}, and write operation.
  
  We set the block size to 512 threads (or the maximum provided for the GPU--512 in the case of our test setup), and then the grid size to $\lceil l/512 \rceil$.
  
  
  \section{Optimizations}
  \subsection{Buffering}
  During our initial proof-of-concept work, we simply read one byte at a time, generated one byte of keystream, sent both to the GPU, then recovered the result. This is slow and inefficient because keystream generation and memory copying are blocking, so for every byte of input data, we make three blocking calls, completely obviating any benefit of GPU acceleration--and significantly slowing the serial implementation as well.
  
  Instead of calculating the key for every byte, we wait until an input buffer is full, calculate the keystream for that buffer, then send the buffer and key to the GPU for processing. Further optimization could also be found in asynchronously generating the keystream while the buffer is filling, however, this would require the use of \texttt{pthreads} on the CPU which we found to be beyond the scope of this project.
  
  \subsection{Asynchronous GPU Transfer}
  
  We further optimized the CPU/GPU transfers by using a multi-stream transfer/compute interleaving technique\cite{gpu-conf}. We set up two streams, $S_1$ and $S_2$. At each `cycle', we:
  
  \begin{itemize}
    \item Asynchronously start the transfer of data to the GPU from $A_t$ and $A_k$ on the CPU using stream $S_1$.
    \item Asynchronously recover encrypted data from $A_t$ on the GPU using stream $S_1$.
    \item Asynchronously invoke a kernel which encrypts the data already on the GPU using stream $S_2$.
    \item Capture $l$ bytes of fresh material into a buffer on the host.
  \end{itemize}
  
  We then wait for both streams to complete, then output the data recovered from the GPU.
  
  This allows the two memory transfers, kernel invocation, and fresh material to happen in an asynchronous, interleaved manner, which hides the substantial latency inherent in transferring data to and from the GPU via the PCIe bus.

  
  \section{Experimental Setup}
  
  We tested this setup on a Tesla T10 Processor with CUDA compute capability 1.3. The Tesla T10 sports a total of 240 CUDA cores, maximally allowing 512 threads per block (\texttt{cuda3.cims.nyu.edu}). Because we took extra steps to hide the latency in the CPU-GPU transfers, we simply used the UNIX \texttt{time} utility to measure the total execution time of our programs.
  We generated a 100MB file of known content--in this case all zeros. We took a baseline of unbuffered serial encryption using our RC4 implementation (found in \texttt{rc4\_serial.c}).
  
  We then ran non-interleaved and interleaved buffered versions of the RC4 algorithm against the same file--although of course we do not provide any information about the file to the code; we simply use \texttt{cat} to stream the data to standard input. We run the parallel code using buffer sizes of $2^9 = 512$, doubling each time until $2^24$ for a total of 15 invocations. We repeated this process several times and took an average of the results.
  
  \section{Results}
  
  The serial version of the code produces quite respectable results by itself; the average time for serial execution was 10.55 seconds, for an average throughput of 9.47MB/sec.
  
  Varying the buffer size had a substantial effect on performance. Small buffer sizes, between 512 bytes and 2048 bytes led to at best parity performance with the serial version; the multi-stream interleaved version in particular took a major performance hit when using small buffer sizes. We speculate that the larger number of round-trips caused major bus contention, leading to a large amount of GPU idle time. Additionally, the GPU was underutilized with these small buffer sizes. Since block sizes were statically set to 512 threads, SMs were left idle when they could have been working.
  
  Buffer sizes $\ge 4096$ bytes lead to a performance improvement of 33-45\% over the equivalent serial version. The effects of interleaving the data transfer and computation are also relevant here; the interleaved version typically yields an extra 2-5\% performance increase relative to serial over its equivalent non-interleaved (na\"\i ve) version.
  
  Complete graphs of our results can be found in Figures 1 and 2.
  
  \begin{figure}
    \includegraphics[width=\columnwidth]{averageruntimes}
    \caption{RC4 runtimes by buffer size}
  \end{figure}
  
  \begin{figure}
    \includegraphics[width=\columnwidth]{improvements}
    \caption{Improvement of parallel runtime over serial}
  \end{figure}
  
  % \begin{table}[H!]
  %   \centering
  %   \begin{tabular}{ c | c | c | c }
  %    & Serial & Par 512 & Par 16.7M \\ \hline
  %   Time & 8.613s & 18.753s & 6.125s \\ 
  %   \% change & - & +117.73\% & -28.89\% \\
  %   \end{tabular}
  %   \caption{Non-Interleaved Performance}
  %   \end{table} 
  % 
  %   \begin{table}
  %   \centering
  %   \begin{tabular}{ c | c | c | c }
  %    & Serial & Par 512 & Par 16.7M \\ \hline
  %   Time & 8.613s & 23.701s & 5.987s \\ 
  %   \% change & - & +175.18\% & -30.49\% \\
  %   \end{tabular}
  %   \caption{Interleaved Performance}
  % \end{table} 
  % 
  % \begin{figure}[H!]
  %   \includegraphics[scale=0.40]{gpuruntimes.eps}
  %   \caption{Run time comparison}
  % \end{figure}
  
  \section{Conclusions}
   The best case performance in our test series yields a marked increase in performance over the serial implementation, even taking into account latency when streaming data on and off the GPU, which is the typical source of consternation in high-throughput GPU applications.  While there are a multitude of unexplored optimizations, such as improving the key generation mechanism with a buffer, the observed increase validates our assumption that despite the transfer overhead between CPU and GPU, there is compelling improvement in the overall runtime to warrant further refinement of the implementation.  As GPU technology develops towards heterogenous systems where CPU and GPU are no longer separated by high-latency buses, the performance gap between our serial implementation and our GPU implementation is expected to significantly increase.

  \section{Additional Notes}
  All experimental code may be found at \url{https://github.com/gdickinson/gpuclass}.
  
  \bibliography{final}{}
  \bibliographystyle{plain}
  
\end{document}